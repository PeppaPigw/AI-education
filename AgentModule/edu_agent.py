from langchain.agents import AgentExecutor, create_react_agent
import logging

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.tools import tool
from tools.language_handler import LanguageHandler
from tools.rag_service import RAGService
from tools.rag_utils import get_context_or_empty
from tools.edu_tools import (
    wikipedia_search,
    define_word,
    calculator,
    current_date,
    current_weekday,
    detect_language,
)
from dotenv import load_dotenv
import os
load_dotenv()
model_name=os.environ.get("model_name")
base_url=os.environ.get("base_url")
api_key=os.environ.get("api_key")

@tool
def rag_search(query: str) -> str:
    """rag_search"""
    try:
        retriever = RAGService().get_retriever()
        return get_context_or_empty(query, retriever)
    except Exception as e:  
        return f"RAG search error: {e}"

DEFAULT_PROMPT = PromptTemplate.from_template(
"""Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
Thought: I now know the final answer
Final Answer: the final answer to the original input question

IMPORTANT GUIDELINES:
1. For simple greetings, directly give Final Answer without using tools
2. When users ask about "PDF", "document", "file", "ææ–™", "æ–‡æ¡£", or "è¿™ä¸ªPDF" content, use rag_search tool
3. If the answer is ALREADY in the provided context (from rag_search results), directly give Final Answer - DO NOT try to use tools again
4. NEVER create fake tool names like "æ— éœ€è°ƒç”¨å·¥å…·" - either use a real tool or give Final Answer directly

Example 1:
Question: hello
Thought: This is a simple greeting, I don't need to use any tools
Final Answer: Hello! How can I assist you today?

Example 2:
Question: è¿™é—¨è¯¾å»ºè®®å­¦ä¹ æ—¶é•¿
Context: ...æœ¬æ•™æçš„æ•™å­¦å¤§çº¦éœ€è¦ 60 å­¦æ—¶ï¼Œå…¶ä¸­æ–¹æ³•æ•™å­¦ä¸ä¸Šæœºå®è·µçš„æ¯”ä¾‹ä¸€èˆ¬ä¸åº”å°‘äº 1:1...
Thought: The answer is already in the provided context - it says the course needs about 60 hours
Final Answer: æ ¹æ®æ•™æï¼Œå®Œæˆæœ¬æ•™æçš„æ•™å­¦å¤§çº¦éœ€è¦60å­¦æ—¶ï¼Œå…¶ä¸­æ–¹æ³•æ•™å­¦ä¸ä¸Šæœºå®è·µçš„æ¯”ä¾‹ä¸€èˆ¬ä¸åº”å°‘äº1:1ã€‚

Example 3:
Question: è¿™ä¸ªPDFçš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ
Thought: The user is asking about PDF content, I should use rag_search to find relevant information
Action: rag_search
Action Input: PDFå†…å®¹
Observation: [Retrieved content from PDF]
Thought: I now have the content from the PDF
Final Answer: æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œ[summarize the content]

Example 4:
Question: What is the capital of France?
Thought: I should search for information about France's capital
Action: wikipedia_search
Action Input: capital of France
Observation: Paris is the capital of France
Thought: I now know the final answer
Final Answer: The capital of France is Paris.

Always answer in {language}. If any tool returns text in a different language, translate it to {language} before giving the final answer.

Begin!

Question: {input}
Thought:{agent_scratchpad}"""
)


def create_agent() -> AgentExecutor:
    tools = [
        wikipedia_search,
        define_word,
        calculator,
        current_date,
        current_weekday,
        detect_language,
        rag_search,
    ]
    
  
    llm = ChatOpenAI(model=model_name, temperature=0,base_url=base_url,api_key=api_key)
    agent = create_react_agent(llm, tools, DEFAULT_PROMPT)
    # ğŸ”¥ ä¿®å¤ï¼šé™åˆ¶max_iterationsä¸º3ï¼Œé¿å…ä¸å¿…è¦çš„é‡å¤
    return AgentExecutor(
        agent=agent, 
        tools=tools, 
        verbose=True,
        max_iterations=3,  # æœ€å¤š3æ¬¡è¿­ä»£
        handle_parsing_errors=True,
        early_stopping_method="force"
    )


def run_agent(
    question: str,
    executor: AgentExecutor | None = None,
    retriever=None,
    return_details: bool = False,
) -> str | tuple[str, bool, bool]:
    executor = executor or create_agent()    
    
    logger = logging.getLogger(__name__)
    
    print("\n" + "="*80)
    print("ğŸ¤” ç”¨æˆ·é—®é¢˜:")
    print(f"   {question}")
    logger.info(f"User Question: {question}")

    used_retriever = False
    if retriever:
        context = get_context_or_empty(question, retriever)
        if context:
            # ğŸ”¥ å¢å¼ºæ—¥å¿—ï¼šè¾“å‡ºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            print(f"\nğŸ“š ä»RAGæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ (é•¿åº¦: {len(context)} å­—ç¬¦):")
            print(f"   {context[:200]}..." if len(context) > 200 else f"   {context}")
            logger.info(f"Retrieved context (length: {len(context)}): {context[:500]}")
            
            question = f"{question}\n\nContext:\n{context}"
            used_retriever = True
        else:
            print("\nâš ï¸  RAGæ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡")
            logger.info("No relevant context found from RAG")

    lang = LanguageHandler.choose_or_detect(question)
    print(f"\nğŸŒ æ£€æµ‹åˆ°çš„è¯­è¨€: {lang}")
    logger.info(f"Detected language: {lang}")
    
    try:
        print("\nğŸ¤– Agentæ­£åœ¨å¤„ç†...")
        print("-" * 80)
        logger.info("Agent processing started")
        
        result = executor.invoke({"input": question, "language": lang})
        
        output = result["output"]
        print("-" * 80)
        print("âœ… Agentå›ç­”:")
        print(f"   {output}")
        logger.info(f"Agent output: {output}")
    except Exception as e:  # pragma: no cover - agent execution errors
        output = f"Agent error: {e}"
        print(f"\nâŒ Agenté”™è¯¯: {e}")
        logger.error(f"Agent error: {e}")

    used_fallback = False

    def _needs_fallback(text: str) -> bool:
        text_stripped = text.strip()
        
        if not text_stripped:
            return True
        
        error_markers = [
            "agent error",
            "agent stopped",
            "i don't know",
            "i cannot",
            "i'm unable",
            "agent é”™è¯¯",
            "ä»£ç†é”™è¯¯",
            "æˆ‘ä¸çŸ¥é“",
            "æˆ‘æ— æ³•",
            "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•",
            "æŠ±æ­‰ï¼Œæˆ‘ä¸",
        ]
        text_lower = text_stripped.lower()
        return any(m in text_lower for m in error_markers)

    if _needs_fallback(output):
        print("\nâš ï¸  æ£€æµ‹åˆ°éœ€è¦fallbackï¼Œç›´æ¥è°ƒç”¨LLM...")
        logger.info("Fallback triggered, calling LLM directly")
        
        llm = ChatOpenAI(model=model_name, temperature=0,base_url=base_url,api_key=api_key)
        try:
            msg = llm.invoke(question)
            output = getattr(msg, "content", str(msg))
            print(f"âœ… Fallback LLMå›ç­”: {output}")
            logger.info(f"Fallback LLM output: {output}")
        except Exception as e: 
            output = f"LLM error: {e}"
            print(f"âŒ Fallback LLMé”™è¯¯: {e}")
            logger.error(f"Fallback LLM error: {e}")
        finally:
            used_fallback = True

    output = LanguageHandler.ensure_language(output, lang)
    print("="*80 + "\n")
    
    if return_details:
        return output, used_fallback, used_retriever
    return output
